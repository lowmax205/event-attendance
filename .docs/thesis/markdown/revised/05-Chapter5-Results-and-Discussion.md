# CHAPTER 5: RESULTS AND DISCUSSION

**[IMPORTANT NOTE TO RESEARCHERS]**

_This chapter presents the results of the system evaluation conducted with BSCS students and stakeholders. Since data collection is ongoing or not yet complete, this document provides a structured template with placeholders marked by **[DATA PLACEHOLDER]** where actual results must be inserted._

_Instructions for completing this chapter:_
_1. Replace all **[DATA PLACEHOLDER]** markers with actual data from your survey, interviews, and system logs_
_2. Complete all tables with real statistical values_
_3. Insert actual quotes from participants in the qualitative sections_
_4. Create the figures specified in the placeholders using your actual data_
_5. Analyze your results using the interpretation guidelines provided_
_6. Write the discussion sections based on your actual findings, connecting them to the literature review_

This chapter presents and discusses the findings from the comprehensive evaluation of the Event Attendance System conducted with Bachelor of Science in Computer Science (BSCS) students at Surigao del Norte State University during the pilot implementation phase. The results are organized according to the evaluation framework described in Chapter 4, beginning with respondent profile analysis, followed by quantitative findings from Likert-scale evaluations, multiple-choice responses, and concluding with qualitative insights from open-ended questions and stakeholder interviews. The discussion section interprets these findings in relation to the research objectives and theoretical frameworks established in earlier chapters.

## 5.1 Presentation of Results

### 5.1.1 Respondent Profile Analysis

The evaluation survey was distributed to all BSCS students who used the Event Attendance System during the pilot phase conducted from **[INSERT START DATE]** to **[INSERT END DATE]**. This section presents the demographic characteristics and event participation patterns of survey respondents.

#### Response Rate

**[DATA PLACEHOLDER]**

- Total eligible participants (BSCS students who used the system): N = **[INSERT NUMBER]**
- Completed survey responses received: n = **[INSERT NUMBER]**
- Response rate: **[CALCULATE: (n/N) × 100]**%
- Incomplete responses excluded: **[INSERT NUMBER]**
- Final sample size for analysis: n = **[INSERT FINAL NUMBER]**

_Note: A response rate of 70% or higher is considered excellent for educational research surveys._

#### Table 4: Respondent Profile Distribution

**[DATA PLACEHOLDER - Create table with actual data]**

| Year Level    | Frequency (n) | Percentage (%) |
| ------------- | ------------- | -------------- |
| 1st Year BSCS | [INSERT]      | [INSERT]       |
| 2nd Year BSCS | [INSERT]      | [INSERT]       |
| 3rd Year BSCS | [INSERT]      | [INSERT]       |
| 4th Year BSCS | [INSERT]      | [INSERT]       |
| **Total**     | **[INSERT]**  | **100.0**      |

**Event Attendance Frequency per Semester**

| Attendance Frequency | Frequency (n) | Percentage (%) |
| -------------------- | ------------- | -------------- |
| 0-1 event            | [INSERT]      | [INSERT]       |
| 2-3 events           | [INSERT]      | [INSERT]       |
| 4-5 events           | [INSERT]      | [INSERT]       |
| 6 or more events     | [INSERT]      | [INSERT]       |
| **Total**            | **[INSERT]**  | **100.0**      |

#### Figure 6: Respondent Year Level Distribution

**[FIGURE PLACEHOLDER]**

_Create a bar chart or pie chart showing the distribution of respondents across year levels. Use the data from Table 4._

_Visualization guidelines:_
_- Bar chart: X-axis = Year level (1st, 2nd, 3rd, 4th), Y-axis = Number of respondents_
_- Pie chart: Show percentage distribution with clear labels and legend_
_- Use distinct colors for each year level_
_- Include data labels showing both frequency (n) and percentage (%)_

#### Interpretation of Profile Data

**[INSTRUCTIONS: After inserting actual data, write 1-2 paragraphs analyzing the respondent profile]**

_Example structure:_

The respondent sample consisted of **[N]** BSCS students representing all year levels. The distribution shows that **[X]**% were first-year students, **[Y]**% second-year, **[Z]**% third-year, and **[W]**% fourth-year students. This distribution is **[describe: roughly proportional/skewed toward/representative of]** the actual BSCS student population at SNSU.

Regarding event participation frequency, **[X]**% of respondents attended **[frequency range]** events per semester, indicating **[high/moderate/low]** engagement with BSCS and CCIS department events. This level of participation suggests that the sample includes both frequent event attendees who have extensive experience with the system and occasional attendees who provide perspectives on first-time user experiences.

### 5.1.2 System Usability and User Experience

This section presents the quantitative findings related to the usability and user experience dimensions of the Event Attendance System, addressing Specific Objective 1 (Event Attendance Management System).

#### Table 5: System Usability and User Experience Evaluation Results

**[DATA PLACEHOLDER - Create table with actual statistical data]**

| Survey Item                                                                           | Mean       | SD         | Interpretation                 |
| ------------------------------------------------------------------------------------- | ---------- | ---------- | ------------------------------ |
| **Q3.** The system's layout made it easy to find what I needed during event check-in. | [X.XX]     | [X.XX]     | [Strongly Agree/Agree/Neutral] |
| **Q4.** On-screen instructions were clear at every step of confirming attendance.     | [X.XX]     | [X.XX]     | [Strongly Agree/Agree/Neutral] |
| **Q5.** Scanning the event QR code worked smoothly without repeated attempts.         | [X.XX]     | [X.XX]     | [Strongly Agree/Agree/Neutral] |
| **Q6.** Capturing a selfie with the event backdrop felt straightforward.              | [X.XX]     | [X.XX]     | [Strongly Agree/Agree/Neutral] |
| **Overall Usability Mean**                                                            | **[X.XX]** | **[X.XX]** | **[Interpretation]**           |

_Interpretation Scale:_
_- Mean 4.21-5.00: Strongly Agree (Excellent usability)_
_- Mean 3.41-4.20: Agree (Good usability)_
_- Mean 2.61-3.40: Neutral (Moderate usability)_
_- Mean 1.81-2.60: Disagree (Poor usability)_
_- Mean 1.00-1.80: Strongly Disagree (Very poor usability)_

**Reliability Analysis:**

**[DATA PLACEHOLDER]**

- Cronbach's Alpha for Usability & User Experience dimension: α = **[X.XXX]**
- Interpretation: **[Excellent (>0.90) / Good (0.80-0.89) / Acceptable (0.70-0.79) / Questionable (<0.70)]**

#### Figure 7: System Usability Ratings

**[FIGURE PLACEHOLDER]**

_Create a horizontal bar chart showing mean ratings for each usability item (Q3-Q6). Include:_
_- Bars showing mean values from 1-5 scale_
_- Error bars showing standard deviation_
_- Reference line at 4.0 (Agree threshold)_
_- Color coding: Green for means ≥4.0, Yellow for 3.0-3.99, Red for <3.0_

#### Interpretation of Usability Findings

**[INSTRUCTIONS: After inserting actual data, write 2-3 paragraphs interpreting the usability results]**

_Example structure:_

The system usability evaluation revealed an overall mean rating of **[X.XX]** (SD = **[X.XX]**), indicating that BSCS students **[strongly agreed/agreed/were neutral about]** the system's ease of use. Among the four usability items, **[highest-rated item]** received the highest rating (M = **[X.XX]**, SD = **[X.XX]**), suggesting that **[explain why this aspect was well-received]**.

Conversely, **[lowest-rated item]** received the lowest rating (M = **[X.XX]**, SD = **[X.XX]**), though still within the **[interpretation range]**. This suggests that **[explain potential reasons for lower rating and implications]**. The relatively **[high/low/moderate]** standard deviation of **[X.XX]** indicates **[consistent/varied]** opinions among respondents regarding **[specific aspect]**.

These findings align with Human-Computer Interaction (HCI) principles discussed in Chapter 3, particularly regarding **[specific HCI principle that findings support/challenge]**. The **[high/moderate/low]** usability ratings validate the design decisions made during the prototyping phase, specifically **[mention specific design features that contributed to usability]**.

### 5.1.3 Efficiency and Performance

This section examines the system's efficiency and performance characteristics, addressing the time-saving and responsiveness objectives of the study.

#### Table 6: Efficiency and Performance Metrics

**[DATA PLACEHOLDER - Create table with actual statistical data]**

| Survey Item                                                                                    | Mean       | SD         | Interpretation                 |
| ---------------------------------------------------------------------------------------------- | ---------- | ---------- | ------------------------------ |
| **Q7.** My attendance status updated in real time after submission.                            | [X.XX]     | [X.XX]     | [Strongly Agree/Agree/Neutral] |
| **Q8.** GPS location verification completed within a reasonable time.                          | [X.XX]     | [X.XX]     | [Strongly Agree/Agree/Neutral] |
| **Q9.** The system stayed responsive even during peak check-in periods.                        | [X.XX]     | [X.XX]     | [Strongly Agree/Agree/Neutral] |
| **Q10.** The process of scanning the QR code and completing the attendance check-in was quick. | [X.XX]     | [X.XX]     | [Strongly Agree/Agree/Neutral] |
| **Overall Efficiency Mean**                                                                    | **[X.XX]** | **[X.XX]** | **[Interpretation]**           |

**Reliability Analysis:**

**[DATA PLACEHOLDER]**

- Cronbach's Alpha for Efficiency & Performance dimension: α = **[X.XXX]**

#### System Performance Logs

**[DATA PLACEHOLDER - Extract actual data from system logs]**

| Performance Metric                                   | Value          |
| ---------------------------------------------------- | -------------- |
| Average check-in completion time                     | [X.XX] seconds |
| Median check-in completion time                      | [X.XX] seconds |
| Average QR scan time                                 | [X.XX] seconds |
| Average GPS verification time                        | [X.XX] seconds |
| Average photo upload time                            | [X.XX] seconds |
| Average signature capture time                       | [X.XX] seconds |
| System uptime during evaluation period               | [XX.X]%        |
| Failed submission rate                               | [X.XX]%        |
| Retry rate (submissions requiring multiple attempts) | [X.XX]%        |

#### Figure 8: Efficiency Performance Comparison

**[FIGURE PLACEHOLDER]**

_Create a grouped bar chart comparing:_
_- Manual system average check-in time vs. Automated system average check-in time_
_- Include breakdown by verification steps_
_- Show percentage improvement_
_- Use different colors for manual vs. automated_

#### Interpretation of Efficiency Findings

**[INSTRUCTIONS: Write 2-3 paragraphs interpreting efficiency results]**

_Discuss:_
_- Whether the system met the performance targets established in Chapter 4 (e.g., <60 seconds check-in time)_
_- Comparison with manual system times (if baseline data available)_
_- Identification of bottlenecks or delays_
_- Impact of network connectivity and device variations_
_- Alignment with cloud computing principles (scalability, real-time processing)_

### 5.1.4 Security and Fraud Prevention

This section evaluates students' confidence in the system's security measures and fraud prevention capabilities, directly addressing Specific Objective 3 (Location-Based and Visual Verification Features).

#### Table 7: Security and Fraud Prevention Assessment

**[DATA PLACEHOLDER - Create table with actual statistical data]**

| Survey Item                                                                                                     | Mean       | SD         | Interpretation                 |
| --------------------------------------------------------------------------------------------------------------- | ---------- | ---------- | ------------------------------ |
| **Q11.** Combining QR, GPS, and selfie verification increased my confidence that proxy attendance is prevented. | [X.XX]     | [X.XX]     | [Strongly Agree/Agree/Neutral] |
| **Q12.** I felt that my personal data (photo, location, and signature) was transmitted and stored securely.     | [X.XX]     | [X.XX]     | [Strongly Agree/Agree/Neutral] |
| **Q13.** Permission prompts for camera and location access were transparent and easy to understand.             | [X.XX]     | [X.XX]     | [Strongly Agree/Agree/Neutral] |
| **Q14.** I trust the system to stop unauthorized changes to attendance records.                                 | [X.XX]     | [X.XX]     | [Strongly Agree/Agree/Neutral] |
| **Overall Security Mean**                                                                                       | **[X.XX]** | **[X.XX]** | **[Interpretation]**           |

**Reliability Analysis:**

**[DATA PLACEHOLDER]**

- Cronbach's Alpha for Security & Fraud Prevention dimension: α = **[X.XXX]**

#### Figure 9: Security Features Confidence Level

**[FIGURE PLACEHOLDER]**

_Create a radar/spider chart showing mean ratings for all four security items (Q11-Q14). This visualization helps identify which security aspects inspire most/least confidence._

#### Interpretation of Security Findings

**[INSTRUCTIONS: Write 2-3 paragraphs interpreting security results]**

_Discuss:_
_- Overall confidence in multi-factor verification approach_
_- Privacy concerns (if any) expressed through lower ratings or qualitative feedback_
_- Trust in data handling practices_
_- Perception of fraud prevention effectiveness_
_- Connection to Biometric Authentication Models theory from Chapter 3_

### 5.1.5 Analytics and Reporting Features

This section evaluates the analytics dashboard and reporting capabilities, addressing Specific Objective 4 (Automated Reporting and Data Analytics).

#### Table 8: Analytics and Reporting Features Evaluation

**[DATA PLACEHOLDER - Create table with actual statistical data]**

| Survey Item                                                                                                        | Mean       | SD         | Interpretation                 |
| ------------------------------------------------------------------------------------------------------------------ | ---------- | ---------- | ------------------------------ |
| **Q15.** The real-time dashboard summaries helped me (or my team) monitor attendance effectively during the event. | [X.XX]     | [X.XX]     | [Strongly Agree/Agree/Neutral] |
| **Q16.** The downloadable PDF/CSV reports contained the data needed for post-event documentation.                  | [X.XX]     | [X.XX]     | [Strongly Agree/Agree/Neutral] |
| **Q17.** Visual analytics (e.g., attendance trends, peak check-in times) were easy to interpret.                   | [X.XX]     | [X.XX]     | [Strongly Agree/Agree/Neutral] |
| **Q18.** Accessing the system through a mobile web browser (without installing an app) was convenient.             | [X.XX]     | [X.XX]     | [Strongly Agree/Agree/Neutral] |
| **Overall Analytics Mean**                                                                                         | **[X.XX]** | **[X.XX]** | **[Interpretation]**           |

**Reliability Analysis:**

**[DATA PLACEHOLDER]**

- Cronbach's Alpha for Analytics & Reporting dimension: α = **[X.XXX]**

#### Event Organizer Feedback on Analytics

**[DATA PLACEHOLDER - Insert quotes from USC/organizer interviews]**

_Example:_

USC Representative 1 stated: **"[INSERT ACTUAL QUOTE about dashboard usefulness]"**

Event Organizer 2 commented: **"[INSERT ACTUAL QUOTE about report generation]"**

#### Interpretation of Analytics Findings

**[INSTRUCTIONS: Write 2-3 paragraphs interpreting analytics results]**

_Discuss:_
_- Usefulness of real-time dashboards for event management_
_- Adequacy of generated reports for institutional documentation needs_
_- Interpretability of visual analytics_
_- Preference for mobile web vs. native app_
_- Alignment with cloud computing benefits (accessibility, real-time sync)_

### 5.1.6 Multiple-Choice and System Usage Patterns

This section analyzes responses to multiple-choice questions that provide insights into user experiences and system usage patterns.

#### Question 19: Most Challenging Attendance Step

**[DATA PLACEHOLDER - Create frequency table and chart]**

| Attendance Step              | Frequency (n) | Percentage (%) |
| ---------------------------- | ------------- | -------------- |
| Navigating to the event page | [INSERT]      | [INSERT]       |
| Scanning the QR code         | [INSERT]      | [INSERT]       |
| Granting location permission | [INSERT]      | [INSERT]       |
| Taking the selfie            | [INSERT]      | [INSERT]       |
| Submitting the form          | [INSERT]      | [INSERT]       |
| **Total**                    | **[INSERT]**  | **100.0**      |

#### Figure 10: Most Challenging Attendance Steps

**[FIGURE PLACEHOLDER]**

_Create a horizontal bar chart showing the percentage of students who found each step most challenging._

**Interpretation:**

**[INSTRUCTIONS: Write 1-2 paragraphs analyzing which steps were most challenging and why]**

_Identify the most commonly cited challenge and discuss:_
_- Potential reasons for difficulty_
_- Design improvements to address this challenge_
_- Connection to usability principles_

#### Question 20: Frequency of System Errors

**[DATA PLACEHOLDER - Create frequency table]**

| Error Frequency       | Frequency (n) | Percentage (%) |
| --------------------- | ------------- | -------------- |
| Never                 | [INSERT]      | [INSERT]       |
| Once                  | [INSERT]      | [INSERT]       |
| Two or three times    | [INSERT]      | [INSERT]       |
| More than three times | [INSERT]      | [INSERT]       |
| **Total**             | **[INSERT]**  | **100.0**      |

**Interpretation:**

**[INSTRUCTIONS: Analyze error frequency and system reliability]**

_Calculate percentage experiencing no errors vs. any errors. Discuss implications for system stability._

#### Question 21: Primary Device Used

**[DATA PLACEHOLDER - Create frequency table]**

| Device Type    | Frequency (n) | Percentage (%) |
| -------------- | ------------- | -------------- |
| Android phone  | [INSERT]      | [INSERT]       |
| iPhone         | [INSERT]      | [INSERT]       |
| Tablet         | [INSERT]      | [INSERT]       |
| Laptop/desktop | [INSERT]      | [INSERT]       |
| Other          | [INSERT]      | [INSERT]       |
| **Total**      | **[INSERT]**  | **100.0**      |

#### Figure 11: Device Usage Distribution

**[FIGURE PLACEHOLDER]**

_Create a pie chart showing device distribution with clear percentage labels._

**Interpretation:**

**[INSTRUCTIONS: Discuss device preferences and implications for responsive design]**

_Analyze whether the system successfully accommodates the dominant device types._

### 5.1.7 Qualitative Findings from Open-Ended Questions

This section presents thematic analysis of responses to open-ended questions, providing rich contextual understanding of quantitative results.

#### Question 22: Aspects That Worked Best

**[DATA PLACEHOLDER - Conduct thematic analysis and present themes with supporting quotes]**

**Emerging Themes:**

**Theme 1: [Name of Theme, e.g., "Speed and Convenience"]**

Frequency: **[X]** respondents mentioned this theme

Representative quotes:

- **"[INSERT ACTUAL QUOTE 1]"** (Respondent [ID/Year Level])
- **"[INSERT ACTUAL QUOTE 2]"** (Respondent [ID/Year Level])

Analysis: **[Discuss what this theme reveals about system strengths]**

**Theme 2: [Name of Theme]**

Frequency: **[X]** respondents mentioned this theme

Representative quotes:

- **"[INSERT ACTUAL QUOTE 1]"**
- **"[INSERT ACTUAL QUOTE 2]"**

Analysis: **[Discuss theme implications]**

**Theme 3: [Name of Theme]**

_[Continue for all identified themes]_

#### Question 23: Suggested Improvements and Additional Features

**[DATA PLACEHOLDER - Present themes for improvement suggestions]**

**Emerging Themes:**

**Theme 1: [Name of Theme, e.g., "Network Connectivity Issues"]**

Frequency: **[X]** respondents mentioned this theme

Representative quotes:

- **"[INSERT ACTUAL QUOTE 1]"**
- **"[INSERT ACTUAL QUOTE 2]"**

Recommendations: **[Discuss potential solutions to address this concern]**

**Theme 2: [Name of Theme]**

_[Continue for all identified themes]_

#### Stakeholder Interview Insights

**[DATA PLACEHOLDER - Summarize key insights from USC and administrator interviews]**

**Key Insight 1: [Topic, e.g., "Impact on Administrative Workload"]**

USC President stated: **"[INSERT QUOTE]"**

Analysis: **[Discuss how system affected organizer workflows]**

**Key Insight 2: [Topic]**

_[Continue for major interview themes]_

## 5.2 Discussion of Findings

This section interprets the quantitative and qualitative results presented in Section 5.1, connecting findings to the research objectives, theoretical frameworks, and related literature. The discussion is organized around the research questions posed in Chapter 1.

### Research Question 1: Current Challenges with Traditional Manual Methods

**[INSTRUCTIONS: Discuss what the study revealed about problems with manual attendance]**

_Based on interview data and comparative feedback, discuss:_
_- Specific challenges identified (time consumption, fraud, data loss, processing delays)_
_- How stakeholders described manual system limitations_
_- Connection to literature on manual vs. automated systems_

### Research Question 2: How Multi-Factor Verification Improves Accuracy and Security

**[INSTRUCTIONS: Interpret findings related to QR + GPS + Selfie verification]**

_Discuss:_
_- Effectiveness of multi-factor approach based on security ratings (Table 7)_
_- Student confidence in fraud prevention_
_- Comparison with single-factor systems discussed in literature review_
_- Connection to Biometric Authentication Models (Jain et al., 2004)_
_- Any limitations or challenges with verification methods_

### Research Question 3: Functional and Non-Functional Requirements

**[INSTRUCTIONS: Evaluate whether implemented system met requirements]**

_Discuss:_
_- Which functional requirements were successfully implemented_
_- Achievement of non-functional requirements (performance targets, security standards)_
_- Any requirement gaps identified through evaluation_

### Research Question 4: User Evaluation of Usability, Efficiency, Security, and Analytics

**[INSTRUCTIONS: Synthesize evaluation results across all dimensions]**

_Provide integrated analysis:_
_- Overall system effectiveness based on evaluation metrics_
_- Relative strengths and weaknesses across dimensions_
_- Comparison of ratings across student year levels (if significant differences found)_
_- Triangulation of quantitative ratings with qualitative feedback_

Example structure:

The comprehensive evaluation revealed that BSCS students rated the Event Attendance System favorably across all dimensions. The overall mean ratings ranged from **[lowest dimension]** (M = **[X.XX]**) to **[highest dimension]** (M = **[X.XX]**), all falling within the **[interpretation]** range of the Likert scale. This indicates that the system successfully achieved its primary objectives of providing an accessible, efficient, secure, and feature-rich solution for event attendance tracking.

**Usability and User Experience** emerged as **[a particular strength/area needing improvement]**, with **[X]**% of respondents rating usability items as Agree or Strongly Agree. This aligns with the emphasis on Human-Computer Interaction principles during the design phase, particularly **[specific HCI principle]**. However, **[identify any usability challenges from qualitative data]** suggests opportunities for refinement in **[specific area]**.

**Efficiency and Performance** results **[met/exceeded/fell short of]** the established target of less than 60 seconds per check-in. System logs indicate an average completion time of **[X.XX]** seconds, representing a **[X]**% **[improvement/increase]** compared to the estimated manual system time of **[X]** seconds. The real-time update capability received particularly **[positive/mixed]** feedback, with **[X]**% of students agreeing that their attendance status updated immediately after submission. These findings validate the cloud computing architecture's ability to provide responsive, scalable performance.

**Security and Fraud Prevention** was rated **[highly/moderately/poorly]**, suggesting that students **[have confidence in/are uncertain about]** the multi-factor verification approach. The mean rating of **[X.XX]** for the item on proxy attendance prevention indicates that **[interpretation]**. This aligns with the Biometric Authentication Models discussed in Chapter 3, which emphasize that **[connection to theory]**. Qualitative feedback **[reinforced/challenged]** these ratings, with students commenting that **[summarize relevant quotes]**.

**Analytics and Reporting** features received **[positive/mixed]** evaluations, particularly from event organizers who noted that **[summarize organizer feedback]**. The convenience of mobile web access (Q18) was rated **[highly/moderately]**, addressing concerns raised in the literature about native app download barriers.

### Research Question 5: Reduction of Fraud, Administrative Burden, and Processing Time

**[INSTRUCTIONS: Analyze comparative effectiveness vs. manual system]**

_Create a comparative analysis table:_

#### Table 9: Comparative Analysis: Manual vs. Automated System

**[DATA PLACEHOLDER]**

| Metric                                  | Manual System | Automated System | Improvement    |
| --------------------------------------- | ------------- | ---------------- | -------------- |
| Average check-in time per student       | [X] seconds   | [Y] seconds      | [Z]% reduction |
| Attendance processing time (post-event) | [X] hours     | [Y] minutes      | [Z]% reduction |
| Data entry errors                       | [X]%          | [Y]%             | [Z]% reduction |
| Reported fraud incidents                | [X] per event | [Y] per event    | [Z]% reduction |
| Staff hours required                    | [X] hours     | [Y] hours        | [Z]% reduction |

_Discuss implications:_
_- Quantify time and resource savings_
_- Discuss fraud prevention effectiveness based on organizer reports_
_- Calculate return on investment (time/effort saved vs. system costs)_

### Integration with Theoretical Frameworks

**Human-Computer Interaction (HCI) Principles:**

The usability findings validate Nielsen's (1993) principles in several ways. **[Discuss specific HCI principles and how findings support or challenge them]**. The **[high/moderate/low]** usability ratings suggest that **[interpretation related to learnability, efficiency, memorability, error prevention]**.

**Biometric Authentication Models:**

The security evaluation results provide empirical support for Jain et al.'s (2004) assertions about biometric authentication effectiveness. **[Discuss how selfie verification and GPS tracking function as biometric/location factors]**. The **[high/moderate/low]** confidence in fraud prevention (M = **[X.XX]**) suggests that **[interpretation]**.

**Cloud Computing Models:**

The system's performance demonstrates several cloud computing benefits outlined by Mell and Grance (2011). **[Discuss evidence of on-demand access, scalability, broad network access, measured service]**. The **[successful/challenged]** real-time synchronization validates the cloud architecture choice.

### Comparison with Related Literature

The findings of this study align with and extend previous research on attendance tracking systems:

**Alignment with RFID and Automated Systems:**

Like the studies by Adderley et al. (2020) and Mrabet & Ait Moussa (2020), this research confirms that automated attendance systems significantly reduce processing time and errors compared to manual methods. However, unlike RFID systems that require specialized hardware, the mobile web-based approach demonstrates that **[discuss cost-effectiveness and accessibility advantages]**.

**Support for Mobile-Based Solutions:**

The findings support Caytuiro-Silva et al.'s (2024) conclusions about mobile and cloud-based systems' effectiveness. The **[high/moderate]** ratings for mobile browser convenience (M = **[X.XX]**) validate the decision to implement a web-based rather than native app solution, addressing the download barriers identified in the literature.

**Extension of Biometric Research:**

While Tsai and Li (2022) demonstrated sophisticated facial recognition systems, this study shows that simpler selfie-based verification combined with GPS tracking can achieve **[comparable/different]** fraud prevention confidence among users. **[Discuss implications for balancing sophistication with accessibility]**.

**Contribution to GPS-Based Verification:**

Building on Chiang et al.'s (2022) GPS and NFC research, this study demonstrates that **[discuss findings about GPS verification effectiveness, limitations encountered, user acceptance]**.

### Limitations Observed During Evaluation

The evaluation revealed several limitations that warrant discussion:

**1. Network Dependency:** **[Discuss actual network-related issues encountered]**

**2. GPS Accuracy:** **[Discuss GPS challenges in specific venues]**

**3. Photo Quality:** **[Discuss selfie-related issues due to lighting, camera quality]**

**4. Sample Limitations:** **[Discuss generalizability concerns from BSCS-only sample]**

### Unexpected Findings

**[INSTRUCTIONS: Discuss any surprising results that differed from expectations or literature]**

_Example topics:_
_- Features that were more/less popular than anticipated_
_- Unexpected challenges or user behaviors_
_- Discrepancies between quantitative ratings and qualitative feedback_

### Implications for Practice

The findings have several practical implications for educational institutions considering similar systems:

**1. Multi-Factor Verification is Acceptable:** **[Discuss user acceptance of multiple verification steps]**

**2. Mobile Web Accessibility is Critical:** **[Discuss importance of browser-based access]**

**3. Real-Time Analytics Add Value:** **[Discuss organizer appreciation for dashboards]**

**4. Training and Support Matter:** **[Discuss onboarding and support needs]**

### Summary of Key Findings

In summary, the evaluation of the Event Attendance System reveals that:

1. **[Key finding 1 related to research objective 1]**
2. **[Key finding 2 related to research objective 2]**
3. **[Key finding 3 related to research objective 3]**
4. **[Key finding 4 related to research objective 4]**
5. **[Overall effectiveness summary]**

These findings demonstrate that the system successfully addresses the problem statement articulated in Chapter 1, providing a reliable, secure, and efficient alternative to traditional manual attendance tracking methods for educational events.

---

**[FINAL NOTE TO RESEARCHERS]**

_After completing this chapter with actual data:_

_1. Ensure all tables contain real data with proper totals and percentages_
_2. Create all specified figures using appropriate visualization software_
_3. Insert actual participant quotes (anonymized appropriately)_
_4. Write interpretation paragraphs that genuinely reflect your findings_
_5. Connect findings to specific literature sources with proper citations_
_6. Discuss both positive results and limitations honestly_
_7. Ensure consistency between quantitative results and qualitative interpretations_
_8. Proofread for accuracy in numbers, calculations, and statistical terminology_

_This chapter should tell the complete story of your evaluation results, providing readers with clear evidence of whether the system achieved its objectives and how users experienced it._
